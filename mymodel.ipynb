{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTransfer Learning for Computer Vision Tutorial\n==============================================\n**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n\nIn this tutorial, you will learn how to train a convolutional neural network for\nimage classification using transfer learning. You can read more about the transfer\nlearning at `cs231n notes <https://cs231n.github.io/transfer-learning/>`__\n\nQuoting these notes,\n\n    In practice, very few people train an entire Convolutional Network\n    from scratch (with random initialization), because it is relatively\n    rare to have a dataset of sufficient size. Instead, it is common to\n    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n    contains 1.2 million images with 1000 categories), and then use the\n    ConvNet either as an initialization or a fixed feature extractor for\n    the task of interest.\n\nThese two major transfer learning scenarios look as follows:\n\n-  **Finetuning the convnet**: Instead of random initializaion, we\n   initialize the network with a pretrained network, like the one that is\n   trained on imagenet 1000 dataset. Rest of the training looks as\n   usual.\n-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n   for all of the network except that of the final fully connected\n   layer. This last fully connected layer is replaced with a new one\n   with random weights and only this layer is trained.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from skimage import io, transform\n",
        "\n",
        "import copy\n",
        "import os.path\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "plt.ion()   # interactive mode\n",
        "from matplotlib.pyplot import imshow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n---------\n\nWe will use torchvision and torch.utils.data packages for loading the\ndata.\n\nThe problem we're going to solve today is to train a model to classify\n**ants** and **bees**. We have about 120 training images each for ants and bees.\nThere are 75 validation images for each class. Usually, this is a very\nsmall dataset to generalize upon, if trained from scratch. Since we\nare using transfer learning, we should be able to generalize reasonably\nwell.\n\nThis dataset is a very small subset of imagenet.\n\n.. Note ::\n   Download the data from\n   `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_\n   and extract it to the current directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Writing a custermized dataset for my data\n",
        "class CommendDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir , label_dir  , phase, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.num_image = glob.glob(os.path.join(root_dir,'*.png'))\n",
        "        self.df_label = pd.read_csv(os.path.join(label_dir,'MPM_Layer0013.csv'))\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.num_image)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lab = self.df_label.iloc[idx]['label']\n",
        "        if self.phase == 'val':\n",
        "            idx = idx+1999\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                str(idx+1)+'.png')\n",
        "        image = io.imread(img_name).transpose((2, 0, 1))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image,lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "# data_transforms = {\n",
        "#     'train': transforms.Compose([\n",
        "#         transforms.RandomResizedCrop(224),\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "#     ]),\n",
        "#     'val': transforms.Compose([\n",
        "#         transforms.Resize(256),\n",
        "#         transforms.CenterCrop(224),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "#     ]),\n",
        "# }\n",
        "\n",
        "data_dir = './CMD_Layer0013/'\n",
        "label_dir = './labels/'\n",
        "image_datasets = {x: CommendDataset(root_dir = os.path.join(data_dir, x),label_dir = os.path.join(label_dir,x),phase = x) for x in ['train', 'val']}\n",
        "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "#               for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a few images\n^^^^^^^^^^^^^^^^^^^^^^\nLet's visualize a few training images so as to understand the data\naugmentations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# def imshow(inp, title=None):\n",
        "#     \"\"\"Imshow for Tensor.\"\"\"\n",
        "#     inp = inp.numpy()\n",
        "#     plt.imshow(inp)\n",
        "#     if title is not None:\n",
        "#         plt.title(title)\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# # Get a batch of training data\n",
        "# inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# # Make a grid from batch\n",
        "# out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model\n------------------\n\nNow, let's write a general function to train a model. Here, we will\nillustrate:\n\n-  Scheduling the learning rate\n-  Saving the best model\n\nIn the following, parameter ``scheduler`` is an LR scheduler object from\n``torch.optim.lr_scheduler``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 9999999\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            epoch_loss = 0\n",
        "            epoch_acc = 0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                inputs = inputs.to(device, dtype=torch.float32)\n",
        "                labels = labels.to(device, dtype=torch.float32)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                epoch_loss += loss * inputs.size(0)\n",
        "                # statistics\n",
        "                # running_loss += loss.item() * inputs.size(0)\n",
        "                # running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = epoch_loss/dataset_sizes[phase]\n",
        "\n",
        "            epoch_acc = epoch_loss.sqrt()\n",
        "\n",
        "            print('{} MSE: {:.4f} RMSE: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc < best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the model predictions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGeneric function to display predictions for a few images\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            labels = labels.to(device, dtype=torch.float32)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format('name'))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finetuning the convnet\n----------------------\n\nLoad a pretrained model and reset final fully connected layer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs,1)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 0/49\n----------\ntrain MSE: 162.0099 RMSE: 12.7283\nval MSE: 15720.0332 RMSE: 125.3796\n\nEpoch 1/49\n----------\ntrain MSE: 99.3222 RMSE: 9.9661\nval MSE: 3923.1514 RMSE: 62.6351\n\nEpoch 2/49\n----------\ntrain MSE: 90.3431 RMSE: 9.5049\nval MSE: 1891.8464 RMSE: 43.4954\n\nEpoch 3/49\n----------\ntrain MSE: 1658.9434 RMSE: 40.7301\nval MSE: 19936.6582 RMSE: 141.1972\n\nEpoch 4/49\n----------\ntrain MSE: 8860.9199 RMSE: 94.1325\nval MSE: 4127.8096 RMSE: 64.2480\n\nEpoch 5/49\n----------\ntrain MSE: 627.9932 RMSE: 25.0598\nval MSE: 7996.9219 RMSE: 89.4255\n\nEpoch 6/49\n----------\ntrain MSE: 635.4999 RMSE: 25.2091\nval MSE: 14255.0762 RMSE: 119.3946\n\nEpoch 7/49\n----------\ntrain MSE: 9685.6201 RMSE: 98.4156\nval MSE: 5992.7324 RMSE: 77.4127\n\nEpoch 8/49\n----------\ntrain MSE: 1555.2343 RMSE: 39.4365\nval MSE: 2213.7583 RMSE: 47.0506\n\nEpoch 9/49\n----------\ntrain MSE: 9251.8975 RMSE: 96.1868\nval MSE: 3009.8491 RMSE: 54.8621\n\nEpoch 10/49\n----------\ntrain MSE: 54.4036 RMSE: 7.3759\nval MSE: 11508.1377 RMSE: 107.2760\n\nEpoch 11/49\n----------\ntrain MSE: 5330.0103 RMSE: 73.0069\nval MSE: 10626.6191 RMSE: 103.0855\n\nEpoch 12/49\n----------\ntrain MSE: 4264.4111 RMSE: 65.3025\nval MSE: 1152.1729 RMSE: 33.9437\n\nEpoch 13/49\n----------\ntrain MSE: 166.6037 RMSE: 12.9075\nval MSE: 3908.9561 RMSE: 62.5216\n\nEpoch 14/49\n----------\ntrain MSE: 6152.9600 RMSE: 78.4408\nval MSE: 3032.5483 RMSE: 55.0686\n\nEpoch 15/49\n----------\ntrain MSE: 783.3414 RMSE: 27.9882\nval MSE: 24627.5703 RMSE: 156.9317\n\nEpoch 16/49\n----------\ntrain MSE: 6192.7529 RMSE: 78.6940\nval MSE: 19166.3320 RMSE: 138.4425\n\nEpoch 17/49\n----------\ntrain MSE: 8412.0049 RMSE: 91.7170\nval MSE: 16852.7227 RMSE: 129.8180\n\nEpoch 18/49\n----------\ntrain MSE: 1670.7157 RMSE: 40.8744\nval MSE: 10030.1982 RMSE: 100.1509\n\nEpoch 19/49\n----------\ntrain MSE: 5173.4790 RMSE: 71.9269\nval MSE: 6796.2939 RMSE: 82.4396\n\nEpoch 20/49\n----------\ntrain MSE: 5027.2427 RMSE: 70.9031\nval MSE: 3519.9492 RMSE: 59.3292\n\nEpoch 21/49\n----------\ntrain MSE: 8043.9375 RMSE: 89.6880\nval MSE: 11872.0312 RMSE: 108.9589\n\nEpoch 22/49\n----------\ntrain MSE: 63.2181 RMSE: 7.9510\nval MSE: 44527.5312 RMSE: 211.0155\n\nEpoch 23/49\n----------\ntrain MSE: 1152.1632 RMSE: 33.9435\nval MSE: 8493.0986 RMSE: 92.1580\n\nEpoch 24/49\n----------\ntrain MSE: 5763.1772 RMSE: 75.9156\nval MSE: 2814.4756 RMSE: 53.0516\n\nEpoch 25/49\n----------\ntrain MSE: 326.2425 RMSE: 18.0622\nval MSE: 15933.1934 RMSE: 126.2268\n\nEpoch 26/49\n----------\ntrain MSE: 8230.1025 RMSE: 90.7199\nval MSE: 13232.6553 RMSE: 115.0333\n\nEpoch 27/49\n----------\ntrain MSE: 5942.1997 RMSE: 77.0857\nval MSE: 3489.2334 RMSE: 59.0697\n\nEpoch 28/49\n----------\ntrain MSE: 9521.2266 RMSE: 97.5768\nval MSE: 18639.5273 RMSE: 136.5267\n\nEpoch 29/49\n----------\ntrain MSE: 6790.7949 RMSE: 82.4063\nval MSE: 28512.1445 RMSE: 168.8554\n\nEpoch 30/49\n----------\ntrain MSE: 1592.1506 RMSE: 39.9018\nval MSE: 12610.9043 RMSE: 112.2983\n\nEpoch 31/49\n----------\ntrain MSE: 6605.1562 RMSE: 81.2721\nval MSE: 18499.3027 RMSE: 136.0121\n\nEpoch 32/49\n----------\ntrain MSE: 818.0345 RMSE: 28.6013\nval MSE: 17979.6250 RMSE: 134.0881\n\nEpoch 33/49\n----------\ntrain MSE: 6074.7715 RMSE: 77.9408\nval MSE: 20684.0391 RMSE: 143.8195\n\nEpoch 34/49\n----------\ntrain MSE: 6625.3687 RMSE: 81.3964\nval MSE: 8743.2852 RMSE: 93.5055\n\nEpoch 35/49\n----------\ntrain MSE: 323.3649 RMSE: 17.9823\nval MSE: 11380.3945 RMSE: 106.6789\n\nEpoch 36/49\n----------\ntrain MSE: 165.8987 RMSE: 12.8802\nval MSE: 9909.2793 RMSE: 99.5454\n\nEpoch 37/49\n----------\ntrain MSE: 7278.3413 RMSE: 85.3132\nval MSE: 3143.7725 RMSE: 56.0694\n\nEpoch 38/49\n----------\ntrain MSE: 48.6217 RMSE: 6.9729\nval MSE: 3057.8257 RMSE: 55.2976\n\nEpoch 39/49\n----------\ntrain MSE: 1296.0497 RMSE: 36.0007\nval MSE: 3803.4390 RMSE: 61.6720\n\nEpoch 40/49\n----------\ntrain MSE: 488.8261 RMSE: 22.1094\nval MSE: 801.2206 RMSE: 28.3058\n\nEpoch 41/49\n----------\ntrain MSE: 291.0880 RMSE: 17.0613\nval MSE: 9644.9541 RMSE: 98.2087\n\nEpoch 42/49\n----------\ntrain MSE: 825.8778 RMSE: 28.7381\nval MSE: 24495.5840 RMSE: 156.5107\n\nEpoch 43/49\n----------\ntrain MSE: 582.0968 RMSE: 24.1267\nval MSE: 13727.2031 RMSE: 117.1631\n\nEpoch 44/49\n----------\ntrain MSE: 7635.9565 RMSE: 87.3840\nval MSE: 15680.6445 RMSE: 125.2224\n\nEpoch 45/49\n----------\ntrain MSE: 286.1903 RMSE: 16.9172\nval MSE: 10169.5898 RMSE: 100.8444\n\nEpoch 46/49\n----------\ntrain MSE: 1015.2794 RMSE: 31.8634\nval MSE: 2662.2083 RMSE: 51.5966\n\nEpoch 47/49\n----------\ntrain MSE: 1132.9381 RMSE: 33.6591\nval MSE: 11113.1104 RMSE: 105.4187\n\nEpoch 48/49\n----------\ntrain MSE: 666.3806 RMSE: 25.8143\nval MSE: 4627.1152 RMSE: 68.0229\n\nEpoch 49/49\n----------\ntrain MSE: 1084.4291 RMSE: 32.9307\nval MSE: 12534.5146 RMSE: 111.9576\n\nTraining complete in 14m 14s\nBest val Acc: 28.305840\n"
        }
      ],
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (name dim0, name dim1)\n * (int dim0, int dim1)\n",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-108-72e004aa6e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-107-7eda94712803>\u001b[0m in \u001b[0;36mvisualize_model\u001b[1;34m(model, num_images)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 imshow(inputs.cpu().data[j].transpose((2, 0, 1))\n\u001b[0m\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (name dim0, name dim1)\n * (int dim0, int dim1)\n"
          ]
        }
      ],
      "source": [
        "visualize_model(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ConvNet as fixed feature extractor\n----------------------------------\n\nHere, we need to freeze all the network except the final layer. We need\nto set ``requires_grad == False`` to freeze the parameters so that the\ngradients are not computed in ``backward()``.\n\nYou can read more about this in the documentation\n`here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train and evaluate\n^^^^^^^^^^^^^^^^^^\n\nOn CPU this will take about half the time compared to previous scenario.\nThis is expected as gradients don't need to be computed for most of the\nnetwork. However, forward does need to be computed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 0/49\n----------\ntrain MSE: 6535.7681 RMSE: 80.8441\nval MSE: 108.8095 RMSE: 10.4312\n\nEpoch 1/49\n----------\ntrain MSE: 6715.2686 RMSE: 81.9467\nval MSE: 5485.1382 RMSE: 74.0617\n\nEpoch 2/49\n----------\ntrain MSE: 2872.3582 RMSE: 53.5944\nval MSE: 2153.4875 RMSE: 46.4057\n\nEpoch 3/49\n----------\ntrain MSE: 6352.4570 RMSE: 79.7023\nval MSE: 6195.6016 RMSE: 78.7121\n\nEpoch 4/49\n----------\ntrain MSE: 3571.6323 RMSE: 59.7631\nval MSE: 16527.4688 RMSE: 128.5592\n\nEpoch 5/49\n----------\ntrain MSE: 615.6530 RMSE: 24.8124\nval MSE: 3038.9360 RMSE: 55.1265\n\nEpoch 6/49\n----------\ntrain MSE: 5119.3140 RMSE: 71.5494\nval MSE: 14325.2461 RMSE: 119.6881\n\nEpoch 7/49\n----------\ntrain MSE: 8484.2500 RMSE: 92.1100\nval MSE: 2146.0859 RMSE: 46.3259\n\nEpoch 8/49\n----------\ntrain MSE: 2395.1133 RMSE: 48.9399\nval MSE: 9014.4736 RMSE: 94.9446\n\nEpoch 9/49\n----------\ntrain MSE: 514.3694 RMSE: 22.6797\nval MSE: 976.9918 RMSE: 31.2569\n\nEpoch 10/49\n----------\ntrain MSE: 4620.6504 RMSE: 67.9754\nval MSE: 12757.5117 RMSE: 112.9492\n\nEpoch 11/49\n----------\ntrain MSE: 14525.8828 RMSE: 120.5234\nval MSE: 9392.8018 RMSE: 96.9165\n\nEpoch 12/49\n----------\ntrain MSE: 524.4114 RMSE: 22.9000\nval MSE: 1691.8875 RMSE: 41.1326\n\nEpoch 13/49\n----------\ntrain MSE: 15741.1387 RMSE: 125.4637\nval MSE: 18366.9512 RMSE: 135.5247\n\nEpoch 14/49\n----------\ntrain MSE: 1347.3522 RMSE: 36.7063\nval MSE: 11351.0332 RMSE: 106.5412\n\nEpoch 15/49\n----------\ntrain MSE: 740.3215 RMSE: 27.2088\nval MSE: 26618.0859 RMSE: 163.1505\n\nEpoch 16/49\n----------\ntrain MSE: 276.8315 RMSE: 16.6383\nval MSE: 3906.9297 RMSE: 62.5054\n\nEpoch 17/49\n----------\ntrain MSE: 6510.0640 RMSE: 80.6850\nval MSE: 12289.2236 RMSE: 110.8568\n\nEpoch 18/49\n----------\ntrain MSE: 285.9066 RMSE: 16.9088\nval MSE: 24810.2578 RMSE: 157.5127\n\nEpoch 19/49\n----------\ntrain MSE: 463.2498 RMSE: 21.5232\nval MSE: 1381.0356 RMSE: 37.1623\n\nEpoch 20/49\n----------\ntrain MSE: 279.9420 RMSE: 16.7315\nval MSE: 2398.2197 RMSE: 48.9716\n\nEpoch 21/49\n----------\ntrain MSE: 412.1571 RMSE: 20.3017\nval MSE: 6935.1807 RMSE: 83.2777\n\nEpoch 22/49\n----------\ntrain MSE: 3580.5190 RMSE: 59.8374\nval MSE: 25575.9785 RMSE: 159.9249\n\nEpoch 23/49\n----------\ntrain MSE: 1342.7238 RMSE: 36.6432\nval MSE: 7528.8701 RMSE: 86.7691\n\nEpoch 24/49\n----------\ntrain MSE: 6533.9238 RMSE: 80.8327\nval MSE: 20888.3652 RMSE: 144.5281\n\nEpoch 25/49\n----------\ntrain MSE: 565.3424 RMSE: 23.7769\nval MSE: 3136.9749 RMSE: 56.0087\n\nEpoch 26/49\n----------\ntrain MSE: 8930.4561 RMSE: 94.5011\nval MSE: 1059.9048 RMSE: 32.5562\n\nEpoch 27/49\n----------\ntrain MSE: 543.8980 RMSE: 23.3216\nval MSE: 2657.4573 RMSE: 51.5505\n\nEpoch 28/49\n----------\ntrain MSE: 622.3069 RMSE: 24.9461\nval MSE: 658.5823 RMSE: 25.6629\n\nEpoch 29/49\n----------\ntrain MSE: 453.4825 RMSE: 21.2951\nval MSE: 11307.9434 RMSE: 106.3388\n\nEpoch 30/49\n----------\ntrain MSE: 1888.9305 RMSE: 43.4618\nval MSE: 7158.9102 RMSE: 84.6103\n\nEpoch 31/49\n----------\ntrain MSE: 622.2537 RMSE: 24.9450\nval MSE: 25578.4590 RMSE: 159.9327\n\nEpoch 32/49\n----------\ntrain MSE: 7292.6909 RMSE: 85.3973\nval MSE: 27553.5859 RMSE: 165.9927\n\nEpoch 33/49\n----------\ntrain MSE: 974.1008 RMSE: 31.2106\nval MSE: 1417.1425 RMSE: 37.6450\n\nEpoch 34/49\n----------\ntrain MSE: 103.1844 RMSE: 10.1580\nval MSE: 1185.3033 RMSE: 34.4282\n\nEpoch 35/49\n----------\ntrain MSE: 7332.7485 RMSE: 85.6315\nval MSE: 11368.5781 RMSE: 106.6235\n\nEpoch 36/49\n----------\ntrain MSE: 5457.9609 RMSE: 73.8780\nval MSE: 13887.0312 RMSE: 117.8432\n\nEpoch 37/49\n----------\ntrain MSE: 1697.7742 RMSE: 41.2041\nval MSE: 14762.6699 RMSE: 121.5017\n\nEpoch 38/49\n----------\ntrain MSE: 963.4785 RMSE: 31.0399\nval MSE: 2382.3770 RMSE: 48.8096\n\nEpoch 39/49\n----------\ntrain MSE: 7456.1040 RMSE: 86.3487\nval MSE: 8245.5498 RMSE: 90.8050\n\nEpoch 40/49\n----------\ntrain MSE: 594.9354 RMSE: 24.3913\nval MSE: 13244.9805 RMSE: 115.0868\n\nEpoch 41/49\n----------\ntrain MSE: 316.0768 RMSE: 17.7785\nval MSE: 10770.6660 RMSE: 103.7818\n\nEpoch 42/49\n----------\ntrain MSE: 820.2988 RMSE: 28.6409\nval MSE: 14656.8477 RMSE: 121.0655\n\nEpoch 43/49\n----------\ntrain MSE: 1037.8060 RMSE: 32.2150\nval MSE: 1529.1809 RMSE: 39.1047\n\nEpoch 44/49\n----------\ntrain MSE: 1153.2017 RMSE: 33.9588\nval MSE: 3857.4282 RMSE: 62.1082\n\nEpoch 45/49\n----------\ntrain MSE: 1870.1530 RMSE: 43.2453\nval MSE: 15832.9902 RMSE: 125.8292\n\nEpoch 46/49\n----------\ntrain MSE: 300.9497 RMSE: 17.3479\nval MSE: 10826.3799 RMSE: 104.0499\n\nEpoch 47/49\n----------\ntrain MSE: 9455.1631 RMSE: 97.2377\nval MSE: 1349.4634 RMSE: 36.7350\n\nEpoch 48/49\n----------\ntrain MSE: 573.6861 RMSE: 23.9517\nval MSE: 11429.1504 RMSE: 106.9072\n\nEpoch 49/49\n----------\ntrain MSE: 135.9275 RMSE: 11.6588\nval MSE: 17861.5352 RMSE: 133.6470\n\nTraining complete in 6m 56s\nBest val Acc: 10.431180\n"
        }
      ],
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_model(model_conv)\n\nplt.ioff()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further Learning\n-----------------\n\nIf you would like to learn more about the applications of transfer learning,\ncheckout our `Quantized Transfer Learning for Computer Vision Tutorial <https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_.\n\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python_defaultSpec_1597447381579"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}